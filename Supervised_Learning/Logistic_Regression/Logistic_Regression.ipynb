{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea6d4329",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "Logistic regression is a statistical method used for analyzing a dataset in which there are one or more independent variables that determine an outcome. The outcome is measured with a dichotomous variable (where there are only two possible outcomes). It is used extensively in various fields, including machine learning, most notably for binary classification problems.\n",
    "\n",
    "## 1. Logistic Regression\n",
    "\n",
    "Logistic Regression is used for binary classification problems. It predicts the probability of an event occurrence based on the given independent variables. The core of logistic regression is the logistic function, represented as:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "where $\\sigma(z)$ is the probability of the event occurrence, $e$ is the base of the natural logarithm, and $ z $ is the linear combination of the input features, given by:\n",
    "\n",
    "$$\n",
    "z = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_nx_n\n",
    "$$\n",
    "\n",
    "In this equation,$ \\beta_0, \\beta_1, \\ldots, \\beta_n$ are the parameters of the model, where $ \\beta_0 $ is the intercept and $ \\beta_1, \\beta_2, \\ldots, \\beta_n $ are the coefficients of the respective input features $ x_1, x_2, \\ldots, x_n $.\n",
    "\n",
    "## 2. The Binary Cross Entropy (BCE) Loss Function\n",
    "\n",
    "### 2.1 Likelihood Function\n",
    "In a binary classification problem, we model the probability of observing the data given the parameters of the model. This is called the likelihood function. For a set of independent observations, the likelihood L is the product of individual probabilities:\n",
    "\n",
    "$$L(\\theta) = \\prod_{i=1}^{N} p_i^{y_i} (1 - p_i)^{1 - y_i}$$\n",
    "\n",
    "* $ L(\\theta)$ is the likelihood of the parameters $ \\theta $. \n",
    "* The product $ \\prod $ runs over all observations $ i$ from 1 to $ N $. \n",
    "* $ p_i $ is the predicted probability for the $ i $-th observation.\n",
    "* $ y_i$ is the actual label for the $ i $-th observation, which can be either 0 or 1. \n",
    "* $ p_i^{y_i} $ and $(1 - p_i)^{1 - y_i} $ represent the probability of the $ i $-th observation being correctly classified, raised to the power of the actual label and its complement, respectively. \n",
    "\n",
    "### 2.2 Log-Likelihood\n",
    "We often use the log of the likelihood because it transforms the product into a sum, making it easier to handle, especially computationally.\n",
    "The log-likelihood $ \\log(L(\\theta)) $ becomes:\n",
    "$$\n",
    "\\log(L(\\theta)) = \\sum_{i=1}^{N} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right]\n",
    "$$\n",
    "\n",
    "### 2.3 Negative Log-Likelihood\n",
    "Maximizing the log-likelihood is equivalent to minimizing its negative. Therefore, the objective function in logistic regression, representing the loss, is the negative log-likelihood:\n",
    "$$\n",
    "-\\log(L(\\theta)) = -\\sum_{i=1}^{N} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right]\n",
    "$$\n",
    "This comes to **Binary Cross Entropy Loss function**.\n",
    "\n",
    "### 2.4 Binary Cross Entropy Loss Function\n",
    "The Binary Cross Entropy (BCE) Loss Function, also known as log loss, is widely used in binary classification models. It measures the performance of a model whose output is a probability value between 0 and 1. The BCE loss quantifies the difference between the predicted probabilities and the actual labels. It is defined as follows:\n",
    "\n",
    "$$\n",
    "\\text{BCE Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\cdot \\log(p_i) + (1 - y_i) \\cdot \\log(1 - p_i) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    " * N  is the number of observations.\n",
    " * $y_i$  is the actual label of the $i $-th observation, which can be 0 or 1.\n",
    " * $ p_i$ is the predicted probability of the $ i $-th observation being classified as class 1.\n",
    " * The log function refers to the natural logarithm.\n",
    "\n",
    "This function penalizes predictions that are confidently incorrect more heavily, due to its logarithmic components. As the predicted probability diverges from the actual label, the loss increases exponentially, which is a key characteristic for effective classification models.\n",
    "\n",
    "* The term $y_i \\cdot \\log(p_i)$ contributes to the loss if the actual label is 1. This part of the formula calculates the loss when the true label $ y_i $ is 1. The loss increases as the predicted probability $ p_i $ diverges from 1.\n",
    "* The term $(1 - y_i) \\cdot \\log(1 - p_i) $ contributes to the loss if the actual label is 0. This part calculates the loss for the instances where the true label $ y_i $ is 0. The loss increases as the predicted probability $p_i $ moves away from 0.\n",
    "\n",
    "## 3. Gradient Descent \n",
    "We again implement gradient Descent as the optimization algorithm used for minimizing the Binary Cross Entropy Loss Function.\n",
    "\n",
    "### 3.1 Derivative of BCE\n",
    "The Binary Cross Entropy Loss for a single observation is given by:\n",
    "$$\n",
    "\\text{BCE Loss} = -\\left[y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})\\right]\n",
    "$$\n",
    "\n",
    "The derivative of the sigmoid function $ \\sigma(z) $ with respect to $ z $ is:\n",
    "$$\n",
    "\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))\n",
    "$$\n",
    "The derivative of the BCE Loss with respect to weight $ w_j $ is calculated using the chain rule:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_j} \\text{BCE Loss} = \\frac{\\partial \\text{BCE Loss}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w_j}\n",
    "$$\n",
    "\n",
    "Breaking it down:\n",
    "\n",
    "* $ \\frac{\\partial \\text{BCE Loss}}{\\partial \\hat{y}} $is the derivative of BCE Loss with respect to $ \\hat{y} $.\n",
    "* $ \\frac{\\partial \\hat{y}}{\\partial z} $ is $ \\sigma'(z) $, the derivative of the sigmoid function.\n",
    "* $\\frac{\\partial z}{\\partial w_j} $ is $ x_j $, the $ j $-th input feature.\n",
    "\n",
    "After substituting and simplifying, this becomes:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_j} \\text{BCE Loss} = (\\hat{y} - y) x_j\n",
    "$$\n",
    "\n",
    "\n",
    "For the bias, the derivative is calculated similarly:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial b} \\text{BCE Loss} = \\frac{\\partial \\text{BCE Loss}}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial b}\n",
    "$$\n",
    "Here,$\\frac{\\partial z}{\\partial b} $ is 1, so the derivative simplifies to:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial b} \\text{BCE Loss} = (\\hat{y} - y)\n",
    "$$\n",
    "\n",
    "With above partial derivatives we can train our logistic regression neuron in the exact same way as our previous models by implementing stochastic gradient descent. Now we can build our algorithm.\n",
    "\n",
    "## 4. Data implementation\n",
    "\n",
    "### 4.1 Data visualization and standardization\n",
    "In this section, we utilized a publicly accessible dataset from Kaggle, originating from a continuous cardiovascular research project conducted in Framingham, Massachusetts. This dataset's primary objective is to forecast the 10-year risk of coronary heart disease (CHD) in patients. It contains detailed information on patients, encompassing more than 4,000 entries and 15 different attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2047fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>currentSmoker</th>\n",
       "      <th>cigsPerDay</th>\n",
       "      <th>BPMeds</th>\n",
       "      <th>prevalentStroke</th>\n",
       "      <th>prevalentHyp</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>totChol</th>\n",
       "      <th>sysBP</th>\n",
       "      <th>diaBP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>heartRate</th>\n",
       "      <th>glucose</th>\n",
       "      <th>TenYearCHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.97</td>\n",
       "      <td>80.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>28.73</td>\n",
       "      <td>95.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>127.5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>25.34</td>\n",
       "      <td>75.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>28.58</td>\n",
       "      <td>65.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>23.10</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>30.30</td>\n",
       "      <td>77.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>33.11</td>\n",
       "      <td>60.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>21.68</td>\n",
       "      <td>79.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>141.5</td>\n",
       "      <td>89.0</td>\n",
       "      <td>26.36</td>\n",
       "      <td>76.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>23.61</td>\n",
       "      <td>93.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   male  age  education  currentSmoker  cigsPerDay  BPMeds  prevalentStroke  \\\n",
       "0     1   39        4.0              0         0.0     0.0                0   \n",
       "1     0   46        2.0              0         0.0     0.0                0   \n",
       "2     1   48        1.0              1        20.0     0.0                0   \n",
       "3     0   61        3.0              1        30.0     0.0                0   \n",
       "4     0   46        3.0              1        23.0     0.0                0   \n",
       "5     0   43        2.0              0         0.0     0.0                0   \n",
       "6     0   63        1.0              0         0.0     0.0                0   \n",
       "7     0   45        2.0              1        20.0     0.0                0   \n",
       "8     1   52        1.0              0         0.0     0.0                0   \n",
       "9     1   43        1.0              1        30.0     0.0                0   \n",
       "\n",
       "   prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  heartRate  glucose  \\\n",
       "0             0         0    195.0  106.0   70.0  26.97       80.0     77.0   \n",
       "1             0         0    250.0  121.0   81.0  28.73       95.0     76.0   \n",
       "2             0         0    245.0  127.5   80.0  25.34       75.0     70.0   \n",
       "3             1         0    225.0  150.0   95.0  28.58       65.0    103.0   \n",
       "4             0         0    285.0  130.0   84.0  23.10       85.0     85.0   \n",
       "5             1         0    228.0  180.0  110.0  30.30       77.0     99.0   \n",
       "6             0         0    205.0  138.0   71.0  33.11       60.0     85.0   \n",
       "7             0         0    313.0  100.0   71.0  21.68       79.0     78.0   \n",
       "8             1         0    260.0  141.5   89.0  26.36       76.0     79.0   \n",
       "9             1         0    225.0  162.0  107.0  23.61       93.0     88.0   \n",
       "\n",
       "   TenYearCHD  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           1  \n",
       "4           0  \n",
       "5           0  \n",
       "6           1  \n",
       "7           0  \n",
       "8           0  \n",
       "9           0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "data = pd.read_csv(\"framingham.csv\")\n",
    "data=data.dropna()\n",
    "X = data.iloc[:,1:15]\n",
    "X_scaler = StandardScaler()\n",
    "X = pd.DataFrame(X_scaler.fit_transform(X))\n",
    "y = data[\"TenYearCHD\"]\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "615b8056",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def BCE_loss(y, y_hat):\n",
    "    return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "class Logistic_GD(object):   \n",
    "    \"\"\"\n",
    "    Logistic_GD is a class for performing logistic regression using Stochastic Gradient Descent (SGD).\n",
    "\n",
    "    This class allows for customizable activation and cost functions, making it versatile for different logistic regression scenarios.\n",
    "\n",
    "    Attributes:\n",
    "        activation_function (callable): The activation function used for logistic regression. \n",
    "                                        This function should take a single argument and return a single value.\n",
    "        cost_function (callable): The cost function used for calculating the loss during training. \n",
    "                                  This function should take two arguments (true value and predicted value) \n",
    "                                  and return a single value representing the loss.\n",
    "    \n",
    "    Methods:\n",
    "        logistic_regression_SGD(X, y, learning_rate, iterations): Performs logistic regression on the provided dataset.\n",
    "            Parameters:\n",
    "                X (array-like): Feature dataset.\n",
    "                y (array-like): Target values.\n",
    "                learning_rate (float, optional): Learning rate for SGD. Default is 0.01.\n",
    "                iterations (int, optional): Number of iterations for SGD. Default is 10000.\n",
    "            Returns:\n",
    "                list: A list of loss values for each iteration.\n",
    "\n",
    "    Example:\n",
    "        >>> activation_func = lambda z: 1 / (1 + np.exp(-z)) # Sigmoid function\n",
    "        >>> cost_func = lambda y_true, y_pred: -y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n",
    "        >>> model = Logistic_GD(activation_func, cost_func)\n",
    "        >>> X, y = load_data() # assuming load_data() is a function that loads your data\n",
    "        >>> losses = model.logistic_regression_SGD(X, y)\n",
    "    \"\"\"\n",
    "    def __init__(self, activation_function, cost_function):\n",
    "        self.activation_function = activation_function\n",
    "        self.cost_function = cost_function\n",
    "    def logistic_regression_SGD(self,X, y, learning_rate=0.01, iterations=10000):\n",
    "        m, n = X.shape\n",
    "        weights = np.zeros(n)\n",
    "        bias = 0\n",
    "        self.Loss_=[]\n",
    "        for _ in range(iterations):\n",
    "            loss = 0\n",
    "            for X_i, y_i in zip(X, y):\n",
    "                y_hat = sigmoid(np.dot(X_i, weights) + bias)\n",
    "                dw = (y_hat - y_i) * X_i\n",
    "                db = y_hat - y_i\n",
    "                weights -= learning_rate * dw\n",
    "                bias -= learning_rate * db\n",
    "                y_hat = self.activation_function(np.dot(X_i, weights) + bias)\n",
    "                loss +=  self.cost_function(y_i,y_hat)\n",
    "            self.Loss_.append(loss/m)\n",
    "        return self.Loss_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8b0814e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x158d69a10>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "node = Logistic_GD(sigmoid, BCE_loss)\n",
    "node.logistic_regression_SGD(X, y, learning_rate = 0.01, iterations = 2000)\n",
    "plt.plot(node.Loss_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
